{
  
    
        "post0": {
            "title": "How to Properly Apply Dropout on Recurrent Neural Network?",
            "content": "How to Properly Apply Dropout on Recurrent Neural Network? . Overview . Dropout is a useful tool for regularizing neural network, but applying it on Recurrent Neural Network could be very tricky! This blogpost would introduce 3 dropout techniques that are empirically effective in training Recurrent Neural Network for language models. . The techniques are originally introduced in a paper by Stephen Merity in 2017. Besides the dropout techniques, the paper also introduce a lot of additional regularization techniques such as ASG optimization, activation regularization and temporal activation regularization. . Since then the techniques have been widely used for training language models with RNN. In particular, FastAI community included the whole AWD-LSTM network in its library and further develop a transfer learning framework called ULMFit that is built on top of AWD-LSTM network. . I recently have a leisure time to read through the original paper and find that the dropout techniques include a lot of details that could be hard to digest. Therefore, this blogpost aims to illustrate the dropout techniques augmented with visual diagrams. I hope it could help the readers more easily understand the dropout techniques introduced in the paper. . The 3 dropout techniques I would introduce are (1) dropout on word embedding, (2) dropout on hidden state, and (3) dropout on hidden-to-hidden weight matrix. I would firstly give you a brief introduction to word embedding and recurrent layer. After that, I will highlight each dropout technique one by one. . What is Dropout? . Before diving into the techniques, let’s briefly review what dropout is. Dropout is a famous weapon for fighting against model overfitting. In essence, dropout is a binary mask applied on a layer (usually the layer applied is an activation layer). It randomly zeroes out each activation unit with a specified chance. By doing so it could effectively discourage model from relying on a small subset of neurons but instead utilize more of its neurons, and hence boosting the generalization power of a model. . Dropout typically mask each activation unit independently with a specified probability. This setting has gained a dominant success in Convolution Neural Network but failed miserably in Recurrent Neural Network because such independency seriously hurts RNN from learning long-term dependency relationship. The various dropout techniques I introduce below are essentially to impose different dependency structure on the dropout mask so as to effectively regulate RNN. . insert CNN dropout visuals . A Briefly Introduction Word Embedding . In language model, we usually have a fixed set of vocabulary and we represent each word by a vector. Word embedding is essentially a weight matrix storing all these vectors. Just imagine a word embedding is a compat storage with all vectors vertically packed so that its row is the vocabulary size and its col is the dimension of the vecotr. You could retrieve the vector of a word by its corresponding index. . . Technique 1: Dropout on Word Embedding . Applying dropout on word embedding is anologous to disappearing some words from a vocabulary. To achieve such effect, we could randomly select some words (indexes) from the vocabulary and completely zero out their vectors. Technically, it is equivalent to randomly masking out some rows of the word embedding weight matrix. . . To minimize the operation, such dropout mask only updates once for each mini-batch and it is applied on the word embedding before feed-forward pass. Another details is that the dropout mask on word embedding is not a binary mask. Instead, it scales non-masked weight by a factor of $1/(1-p)$ where $p$ is the probability that a word to be masked out. In other words, a higher $p$ leads to a larger scaling factor on non-masked weight. . A Brief Introduction to Recurrent Layer . Recurrent Neural Network is a composition of recurrent layer(s). There are many variants of RNN and each variant has its unique design of recurrent layer. For example, one of its variant Long Short Term Memory Network (LSTM) introduces forget, input, cell and output gates in its recurrent layer for better performance. To avoid dwelling into those complexity, I will mainly focus on vanilla recurrent layer, but keep in mind that the dropout principles introduced here can be applied to all RNN variants. Also note that the notations used below mainly follow PyTorch documentation. . Recurrent layer is designed to handle sequence data. In contrast to image data, sequence data usually have serial dependency and they have variable length. Recurrent layer processes sequence data across timestep using its shared parameters. Such design is an efficient structure for model parameterization (less parameters to be learned) and capturing essential dependency (memory). . In order to store the essential memory from the preceding timesteps, recurrent layer introduces an activation layer called hidden state $ bold{h_{t-1}}$. At each time step $t$, a recurrent layer receives input vector at current timestep $ bold{x_{t}}$ and hidden state at preceding timestep $ bold{h_{t-1}}$ to generate hidden state at current timestep $ bold{h_t}$. Finally, $ bold{h_t}$ is passed to 2 places - one for output of the recurrent layer and another for the recurrent input of the recurrent layer in the next timestep. . insert RNN diagram overview . Note that $ bold{x_{t}}$ and $ bold{h_{t-1}}$ often have different dimension. To abbreviate, lets denote the input vector’s dimension as input_size and hidden state’s dimension as hidden_size. Mathematically underneath the hood of the recurrent layer, $ bold{x_{t}}$ and $ bold{h_{t-1}}$ are undergoing the following transformation to get $ bold{h_t}$: . ht=f(Wihxt+Whhht−1+bih+bhh) bold{h_{t}} = f( bold{W_{ih} x_{t} + W_{hh} h_{t-1} + b_{ih} + b_{hh}})ht​=f(Wih​xt​+Whh​ht−1​+bih​+bhh​) . We call $ bold{W_{ih}}$ as input-to-hidden weight matrix because it helps transform an input vector into a vector of size hidden_size. By the same token, we call $ bold{W_{hh}}$ as hidden-to-hidden weight matrix. Activation function $f$ is usually a hyperbolic tangent function $tanh$. Lastly, $ bold{b_{ih}}$ and $ bold{b_{hh}}$ are bias terms. . insert inner mechanism of Recurrent layer . The above overview should suffice to introduce the remaining 2 types of dropout techniques for regulating recurrent layers. . Technique 2: Dropout on Hidden State . An intuitive way to regulate recurrent layer is to apply dropout on hidden state. However, there are several caveats we need to notice when doing so: . Only hidden state for output has dropout applied, hidden state for next timestep is free from dropout | For different samples in a mini-batch, they should have different dropout masks applied on their hidden state | For different timesteps of a given sample, the same dropout mask must be applied on its hidden state | In case of multiple recurrent layers, different dropout masks are applied on different hidden states for a given sample | In case of multiple recurrent layers, the hidden state output from the final layer does not have dropout applied | Similar to dropout on word embedding, the dropout masks on hidden state only update once for each mini-batch, before feed-forward pass. . Here are some simple diagrams to illustrate the above details: . several diagrams to illustrate dropout on hidden states . Technique 3: Dropout on Hidden-to-Hidden Weight Matrix . There are empirical studies showing that overfitting easily appear in recurrent connection of a recurrent layer. To combat such overfitting, we could apply dropout on hidden-to-hidden weight matrix. Such dropout could mitigate the chance that the overfitting in one timestep would significantly propagate to the next timestep. Similar to the above, there are a few caveats to note on its details: . Once the dropout mask is applied on the hidden-to-hidden weight matrix, the mask is locked for all samples and all timesteps in a mini-batch | In case of multiple recurrent layers, different dropout masks are applied on hidden-to-hidden weight matrix of different recurrent layers | Again, the dropout mask on hidden-to-hidden weight matrix is only update once for each mini-batch, before feed-forward pass. In the original paper, authors mention that applying dropout on input-to-hidden weight matrix could also help, but applying so on hidden-to-hidden weight matrix already suffice. . Below diagrams illustrate how dropout on hidden-to-hidden weight matrix works in details: . diagrams . Putting All Dropout Techniques Together . The diagrams below best summarize how it looks like when all dropout techniques are applied on a RNN of multiple recurrent layers: . diagrams . Closing Remark . At last, I hope this article could help clear most of the subtleties in the dropout schemes from AWD-LSTM Network. Finally, I hope you enjoy this blogpost! . Reference .",
            "url": "https://riven314.github.io/alexlauwh/markdown/2020/08/03/AWD_LSTM.html",
            "relUrl": "/markdown/2020/08/03/AWD_LSTM.html",
            "date": " • Aug 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://riven314.github.io/alexlauwh/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://riven314.github.io/alexlauwh/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riven314.github.io/alexlauwh/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riven314.github.io/alexlauwh/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}